{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task V: Quantum Graph Neural Network (QGNN) \n",
    "\n",
    "In task II you already worked with a classical GNN. \n",
    "- Describe a possibility for a QGNN circuit, which takes advantage of the graph representation of the data\n",
    "- Implement and draw the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we make use of the [Jraph](https://github.com/deepmind/jraph) library by deepmind and PennyLane for quantum circuits. Google Flax library is also used. Flax repo has [examples](https://github.com/google/flax/tree/main/examples/ogbg_molpcba) which describes how to use Jraph and we make use of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom TFDS dataset\n",
    "\n",
    "A graph representation of the jet data was first created with the help of [tfds dataset](https://www.tensorflow.org/datasets/add_dataset). The custom dataset files is in `jet_dataset` directory. This way, we don't have to worry about loading and batching graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Quantum-Classical Graph Neural Network \n",
    "\n",
    "### Input network\n",
    "First, the input data of size $N \\times 4$ where $N$ is the number of nodes and $4$ are the features (pt, rapidity, azimuthal angle, and pdgid), is embedded in a quantum circuit. We keep the circuit simple and use the same circuit architecture throughout the GNN. The circuit is shown below:\n",
    "\n",
    "![circuit](../images/circ.png)\n",
    "\n",
    "Every feature vector $x$ is multiplied wih a weight $w$ and added with a bias $b$. The result is fed into the circuit and measured. So the embedding output has a size of $N \\times 4$ (same as the input size).\n",
    "\n",
    "### Update functions\n",
    "\n",
    "We will use only two update functions here: Node level and global level. This is due to the fact the we do not have any edge attributes. The update functions are nothing but neural networks. Here we use hybrid neural networks.\n",
    "\n",
    "First a `fully connected layer` is applied followed by `tanh` activation function. Since the result is between [-1,1], we will feed this into the quantum circuit and measure it. One can specify the number of message passing steps needed.\n",
    "\n",
    "### Classification\n",
    "\n",
    "A final `fully connected layer` is applied down to size 1. Binary cross entropy is used a loss function with Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Hybrid Quantum-Classical Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0401 22:40:11.769851 140239509645120 train.py:292] Model GraphNet\n",
      "I0401 22:40:11.772162 140239509645120 xla_bridge.py:355] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "I0401 22:40:11.772270 140239509645120 xla_bridge.py:355] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "I0401 22:40:11.772315 140239509645120 xla_bridge.py:355] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "I0401 22:40:11.772682 140239509645120 xla_bridge.py:355] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "I0401 22:40:11.772755 140239509645120 xla_bridge.py:355] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "W0401 22:40:11.772795 140239509645120 xla_bridge.py:362] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "I0401 22:40:11.772840 140239509645120 train.py:299] JAX host: 0 / 1\n",
      "I0401 22:40:11.772880 140239509645120 train.py:300] JAX local devices: [CpuDevice(id=0)]\n",
      "I0401 22:40:11.773239 140239509645120 local.py:45] Setting task status: process_index: 0, process_count: 1\n",
      "I0401 22:40:11.773365 140239509645120 local.py:50] Created artifact workdir of type ArtifactType.DIRECTORY and value ./logs.\n",
      "I0401 22:40:11.776867 140230396991040 logging_writer.py:80] [Hyperparameters] {'add_self_loops': True, 'add_undirected_edges': True, 'add_virtual_node': True, 'batch_size': 256, 'checkpoint_every_steps': 10000, 'dropout_rate': 0.1, 'eval_every_steps': 100, 'latent_size': 4, 'layer_norm': True, 'learning_rate': 0.001, 'log_every_steps': 100, 'message_passing_steps': 5, 'model': 'GraphNet', 'num_classes': 2, 'num_mlp_layers': 1, 'num_train_steps': 5000, 'optimizer': 'adam', 'skip_connections': True, 'use_edge_model': False}\n",
      "I0401 22:40:11.777831 140239509645120 train.py:195] Obtaining datasets.\n",
      "I0401 22:40:11.784508 140239509645120 dataset_info.py:565] Load dataset info from /home/gopald/tensorflow_datasets/jet_dataset/1.0.0\n",
      "I0401 22:40:11.786342 140239509645120 dataset_builder.py:522] Reusing dataset jet_dataset (/home/gopald/tensorflow_datasets/jet_dataset/1.0.0)\n",
      "I0401 22:40:11.818567 140239509645120 logging_logger.py:49] Constructing tf.data.Dataset jet_dataset for split train, from /home/gopald/tensorflow_datasets/jet_dataset/1.0.0\n",
      "I0401 22:40:11.836804 140239509645120 logging_logger.py:49] Constructing tf.data.Dataset jet_dataset for split val, from /home/gopald/tensorflow_datasets/jet_dataset/1.0.0\n",
      "I0401 22:40:11.855701 140239509645120 logging_logger.py:49] Constructing tf.data.Dataset jet_dataset for split test, from /home/gopald/tensorflow_datasets/jet_dataset/1.0.0\n",
      "2023-04-01 22:40:12.247442: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-04-01 22:40:12.286842: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "I0401 22:40:12.431589 140239509645120 train.py:204] Initializing network.\n",
      "I0401 22:40:16.419785 140239509645120 parameter_overview.py:264] \n",
      "+------------------------------+-----------+------+-----------+---------+\n",
      "| Name                         | Shape     | Size | Mean      | Std     |\n",
      "+------------------------------+-----------+------+-----------+---------+\n",
      "| params/Dense_0/bias          | (2,)      | 2    | 0.0       | 0.0     |\n",
      "| params/Dense_0/kernel        | (4, 2)    | 8    | 0.0123    | 0.468   |\n",
      "| params/HQNN_0/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_0/Dense_0/kernel | (7, 4)    | 28   | 0.0319    | 0.377   |\n",
      "| params/HQNN_0/theta          | (1, 2, 4) | 8    | 0.00771   | 0.0128  |\n",
      "| params/HQNN_1/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_1/Dense_0/kernel | (6, 4)    | 24   | 0.166     | 0.39    |\n",
      "| params/HQNN_1/theta          | (1, 2, 4) | 8    | -0.00181  | 0.00819 |\n",
      "| params/HQNN_2/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_2/Dense_0/kernel | (10, 4)   | 40   | 0.0994    | 0.297   |\n",
      "| params/HQNN_2/theta          | (1, 2, 4) | 8    | -0.00265  | 0.00811 |\n",
      "| params/HQNN_3/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_3/Dense_0/kernel | (9, 4)    | 36   | -0.04     | 0.328   |\n",
      "| params/HQNN_3/theta          | (1, 2, 4) | 8    | -0.0011   | 0.0103  |\n",
      "| params/HQNN_4/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_4/Dense_0/kernel | (10, 4)   | 40   | -0.0831   | 0.294   |\n",
      "| params/HQNN_4/theta          | (1, 2, 4) | 8    | 0.00153   | 0.00667 |\n",
      "| params/HQNN_5/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_5/Dense_0/kernel | (9, 4)    | 36   | -0.037    | 0.359   |\n",
      "| params/HQNN_5/theta          | (1, 2, 4) | 8    | -0.000912 | 0.00936 |\n",
      "| params/HQNN_6/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_6/Dense_0/kernel | (10, 4)   | 40   | 0.0371    | 0.299   |\n",
      "| params/HQNN_6/theta          | (1, 2, 4) | 8    | -0.00571  | 0.0108  |\n",
      "| params/HQNN_7/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_7/Dense_0/kernel | (9, 4)    | 36   | 0.0103    | 0.302   |\n",
      "| params/HQNN_7/theta          | (1, 2, 4) | 8    | 0.0017    | 0.0083  |\n",
      "| params/HQNN_8/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_8/Dense_0/kernel | (10, 4)   | 40   | 0.0187    | 0.367   |\n",
      "| params/HQNN_8/theta          | (1, 2, 4) | 8    | -0.00149  | 0.00756 |\n",
      "| params/HQNN_9/Dense_0/bias   | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/HQNN_9/Dense_0/kernel | (9, 4)    | 36   | -0.0211   | 0.366   |\n",
      "| params/HQNN_9/theta          | (1, 2, 4) | 8    | 0.00496   | 0.00954 |\n",
      "| params/LayerNorm_0/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_0/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_1/bias      | (1,)      | 1    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_1/scale     | (1,)      | 1    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_10/bias     | (1,)      | 1    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_10/scale    | (1,)      | 1    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_11/bias     | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_11/scale    | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_12/bias     | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_12/scale    | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_13/bias     | (1,)      | 1    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_13/scale    | (1,)      | 1    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_14/bias     | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_14/scale    | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_2/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_2/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_3/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_3/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_4/bias      | (1,)      | 1    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_4/scale     | (1,)      | 1    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_5/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_5/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_6/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_6/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_7/bias      | (1,)      | 1    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_7/scale     | (1,)      | 1    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_8/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_8/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/LayerNorm_9/bias      | (4,)      | 4    | 0.0       | 0.0     |\n",
      "| params/LayerNorm_9/scale     | (4,)      | 4    | 1.0       | 0.0     |\n",
      "| params/gp                    | (2, 1)    | 2    | 0.00309   | 0.0112  |\n",
      "| params/np                    | (2, 4)    | 8    | -0.00734  | 0.00708 |\n",
      "+------------------------------+-----------+------+-----------+---------+\n",
      "Total: 586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0401 22:40:16.534985 140239509645120 train.py:234] Starting training.\n",
      "I0401 22:40:33.432315 140239509645120 train.py:254] Finished training step 1.\n",
      "I0401 22:40:33.643307 140239509645120 train.py:254] Finished training step 2.\n",
      "I0401 22:40:33.853262 140239509645120 train.py:254] Finished training step 3.\n",
      "I0401 22:40:34.107882 140239509645120 train.py:254] Finished training step 4.\n",
      "I0401 22:40:34.312197 140239509645120 train.py:254] Finished training step 5.\n",
      "I0401 22:40:34.511267 140239509645120 train.py:254] Finished training step 6.\n",
      "I0401 22:40:34.751202 140239509645120 train.py:254] Finished training step 7.\n",
      "I0401 22:40:34.992794 140239509645120 train.py:254] Finished training step 8.\n",
      "I0401 22:40:35.204037 140239509645120 train.py:254] Finished training step 9.\n",
      "I0401 22:40:35.407538 140239509645120 train.py:254] Finished training step 10.\n",
      "I0401 22:40:38.779051 140239509645120 local.py:50] Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "I0401 22:41:02.223277 140230396991040 logging_writer.py:48] [100] train_accuracy=0.5419238209724426, train_loss=0.7360214591026306\n",
      "I0401 22:41:12.895208 140230396991040 logging_writer.py:48] [100] val_accuracy=0.7092000246047974, val_auc=0.840125, val_loss=0.6039065718650818, val_mean_average_precision=0.831476\n",
      "I0401 22:41:33.486896 140239509645120 local.py:41] Setting work unit notes: 2.9 steps/s, 3.5% (177/5000), ETA: 27m (1m : 13.5% eval)\n",
      "I0401 22:41:33.487262 140230396991040 logging_writer.py:48] [177] steps_per_sec=2.930676\n",
      "I0401 22:41:33.488012 140230396991040 logging_writer.py:48] [177] uptime=76.952149\n",
      "I0401 22:41:38.811756 140230396991040 logging_writer.py:48] [200] train_accuracy=0.580824613571167, train_loss=0.6896235942840576\n",
      "I0401 22:41:41.259550 140230396991040 logging_writer.py:48] [200] val_accuracy=0.6952000260353088, val_auc=0.842763, val_loss=0.6086099147796631, val_mean_average_precision=0.833670\n",
      "I0401 22:42:07.377483 140230396991040 logging_writer.py:48] [300] train_accuracy=0.6114423274993896, train_loss=0.6571009159088135\n",
      "I0401 22:42:09.423670 140230396991040 logging_writer.py:48] [300] val_accuracy=0.6990000009536743, val_auc=0.832199, val_loss=0.5998237729072571, val_mean_average_precision=0.813900\n",
      "I0401 22:42:33.637447 140239509645120 local.py:41] Setting work unit notes: 3.3 steps/s, 7.5% (375/5000), ETA: 23m (2m : 10.8% eval)\n",
      "I0401 22:42:33.638862 140230396991040 logging_writer.py:48] [375] steps_per_sec=3.291741\n",
      "I0401 22:42:33.639629 140230396991040 logging_writer.py:48] [375] uptime=137.102764\n",
      "I0401 22:42:41.678364 140230396991040 logging_writer.py:48] [400] train_accuracy=0.6537278294563293, train_loss=0.6275638937950134\n",
      "I0401 22:42:43.911695 140230396991040 logging_writer.py:48] [400] val_accuracy=0.7213000059127808, val_auc=0.839290, val_loss=0.5647225379943848, val_mean_average_precision=0.830886\n",
      "I0401 22:43:16.892880 140230396991040 logging_writer.py:48] [500] train_accuracy=0.7101004719734192, train_loss=0.5832026600837708\n",
      "I0401 22:43:19.130661 140230396991040 logging_writer.py:48] [500] val_accuracy=0.7311000227928162, val_auc=0.842457, val_loss=0.5448854565620422, val_mean_average_precision=0.833643\n",
      "I0401 22:43:33.703893 140239509645120 local.py:41] Setting work unit notes: 2.9 steps/s, 11.0% (551/5000), ETA: 25m (3m : 9.8% eval)\n",
      "I0401 22:43:33.704236 140230396991040 logging_writer.py:48] [551] steps_per_sec=2.930088\n",
      "I0401 22:43:33.704785 140230396991040 logging_writer.py:48] [551] uptime=197.169144\n",
      "I0401 22:43:47.144280 140230396991040 logging_writer.py:48] [600] train_accuracy=0.7262113094329834, train_loss=0.5575048923492432\n",
      "I0401 22:43:50.052163 140230396991040 logging_writer.py:48] [600] val_accuracy=0.7499499917030334, val_auc=0.840750, val_loss=0.5323813557624817, val_mean_average_precision=0.826344\n",
      "I0401 22:44:17.778645 140230396991040 logging_writer.py:48] [700] train_accuracy=0.7333812117576599, train_loss=0.5459892749786377\n",
      "I0401 22:44:20.406391 140230396991040 logging_writer.py:48] [700] val_accuracy=0.7662500143051147, val_auc=0.843571, val_loss=0.4982459843158722, val_mean_average_precision=0.834476\n",
      "I0401 22:44:33.836864 140239509645120 local.py:41] Setting work unit notes: 3.1 steps/s, 14.8% (739/5000), ETA: 22m (4m : 9.6% eval)\n",
      "I0401 22:44:33.837791 140230396991040 logging_writer.py:48] [739] steps_per_sec=3.126405\n",
      "I0401 22:44:33.838034 140230396991040 logging_writer.py:48] [739] uptime=257.302146\n",
      "I0401 22:44:53.941171 140230396991040 logging_writer.py:48] [800] train_accuracy=0.7447249889373779, train_loss=0.5306016206741333\n",
      "I0401 22:44:56.939063 140230396991040 logging_writer.py:48] [800] val_accuracy=0.7674000263214111, val_auc=0.843897, val_loss=0.49567168951034546, val_mean_average_precision=0.835555\n",
      "I0401 22:45:29.204698 140230396991040 logging_writer.py:48] [900] train_accuracy=0.7441809773445129, train_loss=0.5297853350639343\n",
      "I0401 22:45:31.671171 140230396991040 logging_writer.py:48] [900] val_accuracy=0.7675999999046326, val_auc=0.844010, val_loss=0.4987049400806427, val_mean_average_precision=0.835315\n",
      "I0401 22:45:34.035346 140239509645120 local.py:41] Setting work unit notes: 2.8 steps/s, 18.1% (907/5000), ETA: 24m (5m : 9.5% eval)\n",
      "I0401 22:45:34.035932 140230396991040 logging_writer.py:48] [907] steps_per_sec=2.790768\n",
      "I0401 22:45:34.036401 140230396991040 logging_writer.py:48] [907] uptime=317.500594\n",
      "I0401 22:46:04.295048 140230396991040 logging_writer.py:48] [1000] train_accuracy=0.7426977753639221, train_loss=0.5308967232704163\n",
      "I0401 22:46:06.771543 140230396991040 logging_writer.py:48] [1000] val_accuracy=0.7601000070571899, val_auc=0.843537, val_loss=0.498626172542572, val_mean_average_precision=0.834753\n",
      "I0401 22:46:34.068967 140239509645120 local.py:41] Setting work unit notes: 2.9 steps/s, 21.7% (1083/5000), ETA: 22m (6m : 8.6% eval)\n",
      "I0401 22:46:34.071917 140230396991040 logging_writer.py:48] [1083] steps_per_sec=2.931694\n",
      "I0401 22:46:34.072178 140230396991040 logging_writer.py:48] [1083] uptime=377.534495\n",
      "I0401 22:46:39.288874 140230396991040 logging_writer.py:48] [1100] train_accuracy=0.7500795722007751, train_loss=0.5181161761283875\n",
      "I0401 22:46:42.280750 140230396991040 logging_writer.py:48] [1100] val_accuracy=0.7674000263214111, val_auc=0.843337, val_loss=0.49904847145080566, val_mean_average_precision=0.834604\n",
      "I0401 22:47:15.087184 140230396991040 logging_writer.py:48] [1200] train_accuracy=0.7522709369659424, train_loss=0.5199499130249023\n",
      "I0401 22:47:17.450372 140230396991040 logging_writer.py:48] [1200] val_accuracy=0.7631999850273132, val_auc=0.843352, val_loss=0.49518314003944397, val_mean_average_precision=0.835127\n",
      "I0401 22:47:34.076257 140239509645120 local.py:41] Setting work unit notes: 2.8 steps/s, 25.0% (1250/5000), ETA: 22m (7m : 8.7% eval)\n",
      "I0401 22:47:34.076652 140230396991040 logging_writer.py:48] [1250] steps_per_sec=2.782992\n",
      "I0401 22:47:34.077231 140230396991040 logging_writer.py:48] [1250] uptime=437.541530\n",
      "I0401 22:47:50.748664 140230396991040 logging_writer.py:48] [1300] train_accuracy=0.7525522708892822, train_loss=0.5169684290885925\n",
      "I0401 22:47:53.246523 140230396991040 logging_writer.py:48] [1300] val_accuracy=0.7557500004768372, val_auc=0.844140, val_loss=0.5008233785629272, val_mean_average_precision=0.835490\n",
      "I0401 22:48:26.511437 140230396991040 logging_writer.py:48] [1400] train_accuracy=0.7584236264228821, train_loss=0.5091603398323059\n",
      "I0401 22:48:29.212719 140230396991040 logging_writer.py:48] [1400] val_accuracy=0.7685499787330627, val_auc=0.844319, val_loss=0.49083271622657776, val_mean_average_precision=0.835519\n",
      "I0401 22:48:34.080790 140239509645120 local.py:41] Setting work unit notes: 2.8 steps/s, 28.3% (1416/5000), ETA: 21m (8m : 8.7% eval)\n",
      "I0401 22:48:34.081235 140230396991040 logging_writer.py:48] [1416] steps_per_sec=2.766458\n",
      "I0401 22:48:34.081349 140230396991040 logging_writer.py:48] [1416] uptime=497.546042\n",
      "I0401 22:48:56.804566 140230396991040 logging_writer.py:48] [1500] train_accuracy=0.7584110498428345, train_loss=0.5129014253616333\n",
      "I0401 22:48:59.198795 140230396991040 logging_writer.py:48] [1500] val_accuracy=0.7596499919891357, val_auc=0.844788, val_loss=0.4955369234085083, val_mean_average_precision=0.836445\n",
      "I0401 22:49:23.398571 140230396991040 logging_writer.py:48] [1600] train_accuracy=0.7574087977409363, train_loss=0.5093631744384766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0401 22:49:25.913345 140230396991040 logging_writer.py:48] [1600] val_accuracy=0.7620000243186951, val_auc=0.844237, val_loss=0.49259793758392334, val_mean_average_precision=0.835882\n",
      "I0401 22:49:34.171748 140239509645120 local.py:41] Setting work unit notes: 3.6 steps/s, 32.7% (1633/5000), ETA: 15m (9m : 8.6% eval)\n",
      "I0401 22:49:34.172178 140230396991040 logging_writer.py:48] [1633] steps_per_sec=3.611194\n",
      "I0401 22:49:34.172849 140230396991040 logging_writer.py:48] [1633] uptime=557.637057\n",
      "I0401 22:49:51.384982 140230396991040 logging_writer.py:48] [1700] train_accuracy=0.7631799578666687, train_loss=0.5032984018325806\n",
      "I0401 22:49:53.757784 140230396991040 logging_writer.py:48] [1700] val_accuracy=0.7641500234603882, val_auc=0.844438, val_loss=0.49152037501335144, val_mean_average_precision=0.836055\n",
      "I0401 22:50:16.884170 140230396991040 logging_writer.py:48] [1800] train_accuracy=0.761698842048645, train_loss=0.505814790725708\n",
      "I0401 22:50:19.467192 140230396991040 logging_writer.py:48] [1800] val_accuracy=0.7688999772071838, val_auc=0.844430, val_loss=0.489300936460495, val_mean_average_precision=0.835410\n",
      "I0401 22:50:34.326115 140239509645120 local.py:41] Setting work unit notes: 3.8 steps/s, 37.3% (1863/5000), ETA: 13m (10m : 8.6% eval)\n",
      "I0401 22:50:34.326477 140230396991040 logging_writer.py:48] [1863] steps_per_sec=3.823495\n",
      "I0401 22:50:34.327149 140230396991040 logging_writer.py:48] [1863] uptime=617.791369\n",
      "I0401 22:50:43.426567 140230396991040 logging_writer.py:48] [1900] train_accuracy=0.7602911591529846, train_loss=0.5022344589233398\n",
      "I0401 22:50:46.140866 140230396991040 logging_writer.py:48] [1900] val_accuracy=0.7630000114440918, val_auc=0.844568, val_loss=0.494797945022583, val_mean_average_precision=0.836036\n",
      "I0401 22:51:10.521573 140230396991040 logging_writer.py:48] [2000] train_accuracy=0.7632481455802917, train_loss=0.500918984413147\n",
      "I0401 22:51:12.851881 140230396991040 logging_writer.py:48] [2000] val_accuracy=0.7670000195503235, val_auc=0.844584, val_loss=0.49010077118873596, val_mean_average_precision=0.835446\n",
      "I0401 22:51:34.442045 140239509645120 local.py:41] Setting work unit notes: 3.8 steps/s, 41.8% (2089/5000), ETA: 12m (11m : 8.5% eval)\n",
      "I0401 22:51:34.442784 140230396991040 logging_writer.py:48] [2089] steps_per_sec=3.759403\n",
      "I0401 22:51:34.443007 140230396991040 logging_writer.py:48] [2089] uptime=677.907301\n",
      "I0401 22:51:37.185887 140230396991040 logging_writer.py:48] [2100] train_accuracy=0.7626720070838928, train_loss=0.5023126602172852\n",
      "I0401 22:51:39.762380 140230396991040 logging_writer.py:48] [2100] val_accuracy=0.7670000195503235, val_auc=0.844347, val_loss=0.48947420716285706, val_mean_average_precision=0.835359\n",
      "I0401 22:52:05.487111 140230396991040 logging_writer.py:48] [2200] train_accuracy=0.7652825713157654, train_loss=0.49936217069625854\n",
      "I0401 22:52:08.010991 140230396991040 logging_writer.py:48] [2200] val_accuracy=0.7589499950408936, val_auc=0.844421, val_loss=0.49487432837486267, val_mean_average_precision=0.835502\n",
      "I0401 22:52:34.625192 140239509645120 local.py:41] Setting work unit notes: 3.4 steps/s, 45.9% (2296/5000), ETA: 13m (12m : 8.5% eval)\n",
      "I0401 22:52:34.626572 140230396991040 logging_writer.py:48] [2296] steps_per_sec=3.439501\n",
      "I0401 22:52:34.626692 140230396991040 logging_writer.py:48] [2296] uptime=738.090455\n",
      "I0401 22:52:35.756480 140230396991040 logging_writer.py:48] [2300] train_accuracy=0.7614325881004333, train_loss=0.5038723349571228\n",
      "I0401 22:52:38.494552 140230396991040 logging_writer.py:48] [2300] val_accuracy=0.7675999999046326, val_auc=0.844305, val_loss=0.48957642912864685, val_mean_average_precision=0.835804\n",
      "I0401 22:53:06.765735 140230396991040 logging_writer.py:48] [2400] train_accuracy=0.7672300338745117, train_loss=0.4954846203327179\n",
      "I0401 22:53:09.729112 140230396991040 logging_writer.py:48] [2400] val_accuracy=0.76705002784729, val_auc=0.844170, val_loss=0.4883042871952057, val_mean_average_precision=0.835556\n",
      "I0401 22:53:34.820471 140239509645120 local.py:41] Setting work unit notes: 3.3 steps/s, 49.9% (2496/5000), ETA: 12m (13m : 8.6% eval)\n",
      "I0401 22:53:34.820860 140230396991040 logging_writer.py:48] [2496] steps_per_sec=3.322520\n",
      "I0401 22:53:34.821484 140230396991040 logging_writer.py:48] [2496] uptime=798.285751\n",
      "I0401 22:53:35.863812 140230396991040 logging_writer.py:48] [2500] train_accuracy=0.762080729007721, train_loss=0.49985551834106445\n",
      "I0401 22:53:38.392125 140230396991040 logging_writer.py:48] [2500] val_accuracy=0.7681999802589417, val_auc=0.844411, val_loss=0.4884505867958069, val_mean_average_precision=0.834633\n",
      "I0401 22:54:01.090669 140230396991040 logging_writer.py:48] [2600] train_accuracy=0.7620974183082581, train_loss=0.5007247924804688\n",
      "I0401 22:54:03.501606 140230396991040 logging_writer.py:48] [2600] val_accuracy=0.7681000232696533, val_auc=0.844657, val_loss=0.48827528953552246, val_mean_average_precision=0.836024\n",
      "I0401 22:54:25.667573 140230396991040 logging_writer.py:48] [2700] train_accuracy=0.766844630241394, train_loss=0.49176472425460815\n",
      "I0401 22:54:28.098775 140230396991040 logging_writer.py:48] [2700] val_accuracy=0.7669000029563904, val_auc=0.845051, val_loss=0.492794007062912, val_mean_average_precision=0.836481\n",
      "I0401 22:54:34.850030 140239509645120 local.py:41] Setting work unit notes: 3.9 steps/s, 54.6% (2731/5000), ETA: 9m (14m : 8.9% eval)\n",
      "I0401 22:54:34.851166 140230396991040 logging_writer.py:48] [2731] steps_per_sec=3.914739\n",
      "I0401 22:54:34.851421 140230396991040 logging_writer.py:48] [2731] uptime=858.315351\n",
      "I0401 22:54:50.255688 140230396991040 logging_writer.py:48] [2800] train_accuracy=0.7611432075500488, train_loss=0.5040844082832336\n",
      "I0401 22:54:52.581843 140230396991040 logging_writer.py:48] [2800] val_accuracy=0.7689999938011169, val_auc=0.844243, val_loss=0.4890197813510895, val_mean_average_precision=0.835523\n",
      "I0401 22:55:14.628156 140230396991040 logging_writer.py:48] [2900] train_accuracy=0.7642377018928528, train_loss=0.4987843930721283\n",
      "I0401 22:55:17.020863 140230396991040 logging_writer.py:48] [2900] val_accuracy=0.7688500285148621, val_auc=0.844614, val_loss=0.4879976511001587, val_mean_average_precision=0.836000\n",
      "I0401 22:55:34.878016 140239509645120 local.py:41] Setting work unit notes: 4.2 steps/s, 59.6% (2981/5000), ETA: 8m (15m : 8.8% eval)\n",
      "I0401 22:55:34.878415 140230396991040 logging_writer.py:48] [2981] steps_per_sec=4.164723\n",
      "I0401 22:55:34.879006 140230396991040 logging_writer.py:48] [2981] uptime=918.343300\n",
      "I0401 22:55:39.180168 140230396991040 logging_writer.py:48] [3000] train_accuracy=0.7676604390144348, train_loss=0.49469462037086487\n",
      "I0401 22:55:41.590683 140230396991040 logging_writer.py:48] [3000] val_accuracy=0.7689999938011169, val_auc=0.843544, val_loss=0.48860403895378113, val_mean_average_precision=0.834793\n",
      "I0401 22:56:03.997319 140230396991040 logging_writer.py:48] [3100] train_accuracy=0.7655297517776489, train_loss=0.5007351636886597\n",
      "I0401 22:56:06.431306 140230396991040 logging_writer.py:48] [3100] val_accuracy=0.7692000269889832, val_auc=0.843613, val_loss=0.4884982407093048, val_mean_average_precision=0.834951\n",
      "I0401 22:56:30.042914 140230396991040 logging_writer.py:48] [3200] train_accuracy=0.7640930414199829, train_loss=0.4976602792739868\n",
      "I0401 22:56:32.542554 140230396991040 logging_writer.py:48] [3200] val_accuracy=0.7699000239372253, val_auc=0.844475, val_loss=0.4878532290458679, val_mean_average_precision=0.835968\n",
      "I0401 22:56:35.000093 140239509645120 local.py:41] Setting work unit notes: 3.8 steps/s, 64.2% (3211/5000), ETA: 7m (16m : 9.0% eval)\n",
      "I0401 22:56:35.000527 140230396991040 logging_writer.py:48] [3211] steps_per_sec=3.825550\n",
      "I0401 22:56:35.001172 140230396991040 logging_writer.py:48] [3211] uptime=978.465400\n",
      "I0401 22:56:54.548952 140230396991040 logging_writer.py:48] [3300] train_accuracy=0.7661406397819519, train_loss=0.4958600401878357\n",
      "I0401 22:56:57.023110 140230396991040 logging_writer.py:48] [3300] val_accuracy=0.7678499817848206, val_auc=0.844699, val_loss=0.4887007772922516, val_mean_average_precision=0.836116\n",
      "I0401 22:57:18.611270 140230396991040 logging_writer.py:48] [3400] train_accuracy=0.7663275599479675, train_loss=0.4974675476551056\n",
      "I0401 22:57:21.671626 140230396991040 logging_writer.py:48] [3400] val_accuracy=0.7686499953269958, val_auc=0.844597, val_loss=0.4875205457210541, val_mean_average_precision=0.836156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0401 22:57:35.112144 140239509645120 local.py:41] Setting work unit notes: 4.1 steps/s, 69.2% (3460/5000), ETA: 6m (17m : 9.0% eval)\n",
      "I0401 22:57:35.112529 140230396991040 logging_writer.py:48] [3460] steps_per_sec=4.142264\n",
      "I0401 22:57:35.113233 140230396991040 logging_writer.py:48] [3460] uptime=1038.577412\n",
      "I0401 22:57:43.902540 140230396991040 logging_writer.py:48] [3500] train_accuracy=0.7654215097427368, train_loss=0.4943298101425171\n",
      "I0401 22:57:46.344298 140230396991040 logging_writer.py:48] [3500] val_accuracy=0.7674499750137329, val_auc=0.844710, val_loss=0.48948872089385986, val_mean_average_precision=0.836336\n",
      "I0401 22:58:07.796641 140230396991040 logging_writer.py:48] [3600] train_accuracy=0.765544056892395, train_loss=0.49886035919189453\n",
      "I0401 22:58:10.505812 140230396991040 logging_writer.py:48] [3600] val_accuracy=0.7674499750137329, val_auc=0.844539, val_loss=0.48834970593452454, val_mean_average_precision=0.836222\n",
      "I0401 22:58:31.478740 140230396991040 logging_writer.py:48] [3700] train_accuracy=0.7681584358215332, train_loss=0.49350088834762573\n",
      "I0401 22:58:33.647131 140230396991040 logging_writer.py:48] [3700] val_accuracy=0.7664499878883362, val_auc=0.843837, val_loss=0.4884376525878906, val_mean_average_precision=0.835188\n",
      "I0401 22:58:35.255337 140239509645120 local.py:41] Setting work unit notes: 4.1 steps/s, 74.1% (3707/5000), ETA: 5m (18m : 9.2% eval)\n",
      "I0401 22:58:35.255708 140230396991040 logging_writer.py:48] [3707] steps_per_sec=4.106865\n",
      "I0401 22:58:35.256320 140230396991040 logging_writer.py:48] [3707] uptime=1098.720592\n",
      "I0401 22:58:55.044444 140230396991040 logging_writer.py:48] [3800] train_accuracy=0.7663205862045288, train_loss=0.4964107871055603\n",
      "I0401 22:58:57.471909 140230396991040 logging_writer.py:48] [3800] val_accuracy=0.7681499719619751, val_auc=0.844343, val_loss=0.48772525787353516, val_mean_average_precision=0.835680\n",
      "I0401 22:59:18.902760 140230396991040 logging_writer.py:48] [3900] train_accuracy=0.7634760141372681, train_loss=0.5000028610229492\n",
      "I0401 22:59:21.181616 140230396991040 logging_writer.py:48] [3900] val_accuracy=0.7685999870300293, val_auc=0.843904, val_loss=0.48757442831993103, val_mean_average_precision=0.835679\n",
      "I0401 22:59:35.289367 140239509645120 local.py:41] Setting work unit notes: 4.2 steps/s, 79.2% (3961/5000), ETA: 4m (19m : 9.1% eval)\n",
      "I0401 22:59:35.289772 140230396991040 logging_writer.py:48] [3961] steps_per_sec=4.230936\n",
      "I0401 22:59:35.290351 140230396991040 logging_writer.py:48] [3961] uptime=1158.754652\n",
      "I0401 22:59:43.757102 140230396991040 logging_writer.py:48] [4000] train_accuracy=0.7695356011390686, train_loss=0.48891693353652954\n",
      "I0401 22:59:46.252067 140230396991040 logging_writer.py:48] [4000] val_accuracy=0.767300009727478, val_auc=0.843697, val_loss=0.4901651442050934, val_mean_average_precision=0.835222\n",
      "I0401 23:00:09.429538 140230396991040 logging_writer.py:48] [4100] train_accuracy=0.7650081515312195, train_loss=0.4997296929359436\n",
      "I0401 23:00:11.877672 140230396991040 logging_writer.py:48] [4100] val_accuracy=0.7663999795913696, val_auc=0.844603, val_loss=0.4885043799877167, val_mean_average_precision=0.836242\n",
      "I0401 23:00:35.176866 140230396991040 logging_writer.py:48] [4200] train_accuracy=0.7651301026344299, train_loss=0.49721989035606384\n",
      "I0401 23:00:38.090962 140230396991040 logging_writer.py:48] [4200] val_accuracy=0.7684000134468079, val_auc=0.844555, val_loss=0.4877912998199463, val_mean_average_precision=0.836136\n",
      "I0401 23:00:38.363743 140239509645120 local.py:41] Setting work unit notes: 3.8 steps/s, 84.0% (4201/5000), ETA: 3m (20m : 9.3% eval)\n",
      "I0401 23:00:38.365765 140230396991040 logging_writer.py:48] [4201] steps_per_sec=3.805035\n",
      "I0401 23:00:38.367140 140230396991040 logging_writer.py:48] [4201] uptime=1221.829239\n",
      "I0401 23:01:01.673900 140230396991040 logging_writer.py:48] [4300] train_accuracy=0.7673125267028809, train_loss=0.4895820617675781\n",
      "I0401 23:01:04.455435 140230396991040 logging_writer.py:48] [4300] val_accuracy=0.7706999778747559, val_auc=0.844864, val_loss=0.4871177673339844, val_mean_average_precision=0.836476\n",
      "I0401 23:01:26.425578 140230396991040 logging_writer.py:48] [4400] train_accuracy=0.7649886608123779, train_loss=0.49889734387397766\n",
      "I0401 23:01:28.914374 140230396991040 logging_writer.py:48] [4400] val_accuracy=0.7663000226020813, val_auc=0.844241, val_loss=0.48950931429862976, val_mean_average_precision=0.835449\n",
      "I0401 23:01:38.567352 140239509645120 local.py:41] Setting work unit notes: 4.0 steps/s, 88.9% (4444/5000), ETA: 2m (21m : 9.2% eval)\n",
      "I0401 23:01:38.567748 140230396991040 logging_writer.py:48] [4444] steps_per_sec=4.036298\n",
      "I0401 23:01:38.568431 140230396991040 logging_writer.py:48] [4444] uptime=1282.032620\n",
      "I0401 23:01:51.847687 140230396991040 logging_writer.py:48] [4500] train_accuracy=0.7649681568145752, train_loss=0.49614736437797546\n",
      "I0401 23:01:54.378778 140230396991040 logging_writer.py:48] [4500] val_accuracy=0.7670999765396118, val_auc=0.843521, val_loss=0.4896091818809509, val_mean_average_precision=0.834694\n",
      "I0401 23:02:17.220852 140230396991040 logging_writer.py:48] [4600] train_accuracy=0.7661013603210449, train_loss=0.49460935592651367\n",
      "I0401 23:02:19.632061 140230396991040 logging_writer.py:48] [4600] val_accuracy=0.7615000009536743, val_auc=0.844450, val_loss=0.4934191405773163, val_mean_average_precision=0.835970\n",
      "I0401 23:02:38.629060 140239509645120 local.py:41] Setting work unit notes: 4.0 steps/s, 93.7% (4687/5000), ETA: 1m (22m : 9.2% eval)\n",
      "I0401 23:02:38.629498 140230396991040 logging_writer.py:48] [4687] steps_per_sec=4.045840\n",
      "I0401 23:02:38.629620 140230396991040 logging_writer.py:48] [4687] uptime=1342.094355\n",
      "I0401 23:02:41.422070 140230396991040 logging_writer.py:48] [4700] train_accuracy=0.7662055492401123, train_loss=0.49740979075431824\n",
      "I0401 23:02:44.284097 140230396991040 logging_writer.py:48] [4700] val_accuracy=0.7669500112533569, val_auc=0.845132, val_loss=0.4868096113204956, val_mean_average_precision=0.836980\n",
      "I0401 23:03:05.529786 140230396991040 logging_writer.py:48] [4800] train_accuracy=0.7648417353630066, train_loss=0.4944570064544678\n",
      "I0401 23:03:08.197442 140230396991040 logging_writer.py:48] [4800] val_accuracy=0.765250027179718, val_auc=0.844653, val_loss=0.4895216226577759, val_mean_average_precision=0.836552\n",
      "I0401 23:03:31.615753 140230396991040 logging_writer.py:48] [4900] train_accuracy=0.7694563269615173, train_loss=0.49282652139663696\n",
      "I0401 23:03:34.169103 140230396991040 logging_writer.py:48] [4900] val_accuracy=0.7658500075340271, val_auc=0.844718, val_loss=0.4902506470680237, val_mean_average_precision=0.836266\n",
      "I0401 23:03:38.851380 140239509645120 local.py:41] Setting work unit notes: 3.8 steps/s, 98.4% (4918/5000), ETA: 0m (23m : 9.4% eval)\n",
      "I0401 23:03:38.852222 140230396991040 logging_writer.py:48] [4918] steps_per_sec=3.835786\n",
      "I0401 23:03:38.852602 140230396991040 logging_writer.py:48] [4918] uptime=1402.316629\n",
      "I0401 23:03:58.691792 140230396991040 logging_writer.py:48] [4999] train_accuracy=0.7674128413200378, train_loss=0.49307024478912354\n",
      "I0401 23:04:02.048320 140230396991040 logging_writer.py:48] [4999] val_accuracy=0.7631000280380249, val_auc=0.844692, val_loss=0.49212270975112915, val_mean_average_precision=0.836461\n",
      "I0401 23:04:02.246033 140239509645120 local.py:41] Setting work unit notes: 3.5 steps/s, 100.0% (5000/5000), ETA: 0m (23m : 9.5% eval)\n",
      "I0401 23:04:02.247908 140230396991040 logging_writer.py:48] [5000] steps_per_sec=3.505077\n",
      "I0401 23:04:02.248113 140230396991040 logging_writer.py:48] [5000] uptime=1425.711360\n",
      "I0401 23:04:02.248459 140230396991040 logging_writer.py:48] [5000] train_accuracy=0.7673469185829163, train_loss=0.5190379619598389\n",
      "I0401 23:04:04.535671 140230396991040 logging_writer.py:48] [5000] val_accuracy=0.7648000121116638, val_auc=0.844695, val_loss=0.4902131259441376, val_mean_average_precision=0.836511\n",
      "I0401 23:04:09.037674 140230396991040 logging_writer.py:48] [5000] test_accuracy=0.7661666870117188, test_auc=0.840315, test_loss=0.49338850378990173, test_mean_average_precision=0.828939\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config=configs/default_graph_net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The results were saved with tensorboard in `logs` directory. For convenience, the plots are shown below.\n",
    "\n",
    "![train_loss](../images/train_loss.png)\n",
    "![val_loss](../images/val_loss.png)\n",
    "![train_acc](../images/train_acc.png)\n",
    "![val_acc](../images/val_acc.png)\n",
    "![val_auc](../images/val_auc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We constructed a Hybrid Quantum-Classical Graph Neural Network using Jraph and Pennylane. We were able to achieve 0.84 AUC score on test set and that too without any hyperparmeter tuning. The AUC obtained is closer to the result from [1] (they get an AUC of 0.90) and also with the classical counterpart which we implemented in Task II (0.86 AUC). We need to fine-tune the model with different ansatzes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Komiske, P.T., Metodiev, E.M. & Thaler, J. Energy flow networks: deep sets for particle jets. J. High Energ. Phys. 2019, 121 (2019). https://doi.org/10.1007/JHEP01(2019)121"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qmlhep)",
   "language": "python",
   "name": "qmlhep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
